{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from data_processing.data_generate import generate_random_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.逻辑回归（Logistic Regression）公式\n",
    "\n",
    "逻辑回归模型的输出可以表示为：\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}\n",
    "$$\n",
    "\n",
    "在使用梯度下降法来优化逻辑回归模型时，第一步是明确逻辑回归的目标函数。根据上述公式，逻辑回归的数学形式为 $f_w(x)$。对于一个输入样本 $x$，其预测结果可以是正样本（类别 1）或负样本（类别 0），对应的概率分别为：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "P(y = 1 | x; w) = f_w(x) \\\\\n",
    "P(y = 0 | x; w) = 1 - f_w(x)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "将其联合起来，得到数据点的条件概率表达式：\n",
    "\n",
    "$$\n",
    "p(y | x; w) = (f_w(x))^y(1 - f_w(x))^{1-y}\n",
    "$$\n",
    "\n",
    "**根据极大似然估计的原理，逻辑回归的目标函数可以写成以下形式**。我们的目标是通过优化模型参数 $w$，最大化给定训练数据集的似然函数。换句话说，我们希望通过学习模型参数，使得在该模型下观察到的样本数据出现的概率最大化。\n",
    "\n",
    "$$\n",
    "L(w) = \\prod_{i=1}^{m} P(y_i | x_i; w)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1目标函数\n",
    "\n",
    "逻辑回归的目标函数初始是一个连乘形式，直接对其求导较为复杂。因此，我们通常**对目标函数两侧取对数**，并**乘以系数 $-\\frac{1}{m}$**，这样可以将原本的最大化问题转化为最小化问题。通过取对数，连乘操作变为连加，最终得到的目标函数形式如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(w) &= -\\frac{1}{m}l(w) = -\\frac{1}{m}L(w) \\\\\n",
    "     &= -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y^i \\log f_w(x^i) + (1 - y^i) \\log (1 - f_w(x^i)) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "在得到逻辑回归的目标函数后，接下来的步骤是对每个参数进行偏导数计算，以确定其梯度方向。具体来说，对于目标函数 $J(w)$ 中的参数 $w_j$，我们求其偏导数，结果如下：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_w(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $y^{(i)}$ 代表第 $i$ 个样本的真实标签。\n",
    "\n",
    "- $w_j$ 是逻辑回归模型的参数，而 $x_j^{(i)}$ 则是第 $i$ 个样本的第 $j$ 个特征。\n",
    "\n",
    "损失函数对偏置 b 的偏导数计算如下：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_w(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $f_w(x^{(i)})$ 是第 i 个样本的预测值，计算公式为：  \n",
    "  \n",
    "- $y^{(i)}$ 是第 i 个样本的真实标签。\n",
    "\n",
    "在得到梯度之后，我们利用梯度下降法来更新模型参数。梯度下降的更新规则为：\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_w(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\cdot  \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_w(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "通过以上步骤，我们就完成了逻辑回归模型参数更新的推导过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,x,b):\n",
    "    \"\"\"\n",
    "    计算sigmoid函数\n",
    "    Args:\n",
    "      w (ndarray): 权重参数,n*1\n",
    "      x (ndarray): 样本矩阵,m*n\n",
    "      b (scalar): 偏置项\n",
    "    Returns:\n",
    "      p (ndarray): sigmoid计算结果,m*1\n",
    "    \"\"\"\n",
    "    \n",
    "    z = np.dot(x,w) + b\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b, fw = None):\n",
    "    \"\"\"\n",
    "    计算损失函数\n",
    "    Args:\n",
    "      X (ndarray): 样本矩阵,m*n\n",
    "      y (ndarray): 标签矩阵,m*1\n",
    "      w (ndarray): 权重参数,n*1\n",
    "      b (scalar): 偏置项\n",
    "    Returns:\n",
    "      cost (scalar): 损失函数值\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    #同样使用矩阵计算\n",
    "\n",
    "    #如果已经计算了fw,就不用再计算一遍了sigmoid\n",
    "    if fw is not None :\n",
    "      cost = -(1/m) *  np.sum((y * np.log(fw) + (1 - y) * np.log(1 - fw)))\n",
    "    else:\n",
    "      cost = -(1/m) *  np.sum((y * np.log(sigmoid(w, X, b)) + (1 - y) * np.log(1 - sigmoid(w, X, b))))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logicstic_regression(X, y, learning_rate = 0.01, num_iterations = 1000):\n",
    "    \"\"\"\n",
    "    X: input data\n",
    "    y: output data\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    num_iterations: number of iterations for gradient descent\n",
    "    return: weights and bias\n",
    "    \"\"\"\n",
    "    \n",
    "    rows,cols = X.shape\n",
    "    w = np.random.rand(cols,1)\n",
    "    b = 0 \n",
    "    \n",
    "    pbar = tqdm(range(num_iterations),desc ='Logicstic Regression',ncols = 100,unit = 'it')\n",
    "\n",
    "    for i in pbar:\n",
    "        predictions = sigmoid(w,X,b).astype(np.float32)\n",
    "        cost = compute_cost(X, y, w, b,fw = predictions)\n",
    "        \n",
    "        # 注意这里用矩阵计算,纬度是m*1\n",
    "        error = predictions - y \n",
    "        dw = (1/rows) * np.dot(X.T,error) \n",
    "        db = (1/rows) * np.sum(error)\n",
    "\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        pbar.set_postfix({\"cost\": cost})\n",
    "\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logicstic Regression: 100%|██████████████████████| 1000/1000 [00:00<00:00, 1190.12it/s, cost=0.0357]\n"
     ]
    }
   ],
   "source": [
    "rows,cols = 4,5\n",
    "X = generate_random_matrix(rows = rows, cols = cols, min_value = 1, max_value = 5)\n",
    "y = np.random.randint(0,2, size = (rows,1))\n",
    "\n",
    "\n",
    "w,b = logicstic_regression(X = X, y = y,learning_rate = 0.01, num_iterations = 1000 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0  x1  x2  x3  x4  y    y_pred\n",
       "0   3   4   4   5   2  0  0.054166\n",
       "1   2   3   1   5   5  0  0.000324\n",
       "2   1   1   3   1   5  0  0.020765\n",
       "3   5   4   5   2   2  1  0.936527"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始矩阵\n",
    "prediction = sigmoid(w,X,b)\n",
    "df = pd.DataFrame(np.concatenate((X,y), axis = 1),columns = [f'x{i}' for i in range(X.shape[1])] + ['y'])\n",
    "\n",
    "df['y_pred'] = prediction\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
